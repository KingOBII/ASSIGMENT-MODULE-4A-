{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb58a8d5-f4ce-4be1-b901-4e46866d493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.58516849\n",
      "Iteration 2, loss = 0.51463882\n",
      "Iteration 3, loss = 0.45415566\n",
      "Iteration 4, loss = 0.40317229\n",
      "Iteration 5, loss = 0.36185324\n",
      "Iteration 6, loss = 0.32763286\n",
      "Iteration 7, loss = 0.29905892\n",
      "Iteration 8, loss = 0.27454625\n",
      "Iteration 9, loss = 0.25386692\n",
      "Iteration 10, loss = 0.23670929\n",
      "Iteration 11, loss = 0.22099531\n",
      "Iteration 12, loss = 0.20773776\n",
      "Iteration 13, loss = 0.19619421\n",
      "Iteration 14, loss = 0.18609549\n",
      "Iteration 15, loss = 0.17707466\n",
      "Iteration 16, loss = 0.16907755\n",
      "Iteration 17, loss = 0.16206440\n",
      "Iteration 18, loss = 0.15554023\n",
      "Iteration 19, loss = 0.14979378\n",
      "Iteration 20, loss = 0.14454571\n",
      "Iteration 21, loss = 0.13946892\n",
      "Iteration 22, loss = 0.13512555\n",
      "Iteration 23, loss = 0.13103463\n",
      "Iteration 24, loss = 0.12709115\n",
      "Iteration 25, loss = 0.12350071\n",
      "Iteration 26, loss = 0.12022358\n",
      "Iteration 27, loss = 0.11714537\n",
      "Iteration 28, loss = 0.11418876\n",
      "Iteration 29, loss = 0.11137693\n",
      "Iteration 30, loss = 0.10878287\n",
      "Iteration 31, loss = 0.10630231\n",
      "Iteration 32, loss = 0.10393156\n",
      "Iteration 33, loss = 0.10156596\n",
      "Iteration 34, loss = 0.09955899\n",
      "Iteration 35, loss = 0.09745419\n",
      "Iteration 36, loss = 0.09559913\n",
      "Iteration 37, loss = 0.09360280\n",
      "Iteration 38, loss = 0.09195646\n",
      "Iteration 39, loss = 0.09021838\n",
      "Iteration 40, loss = 0.08847827\n",
      "Iteration 41, loss = 0.08705350\n",
      "Iteration 42, loss = 0.08549137\n",
      "Iteration 43, loss = 0.08401602\n",
      "Iteration 44, loss = 0.08265672\n",
      "Iteration 45, loss = 0.08134134\n",
      "Iteration 46, loss = 0.08000453\n",
      "Iteration 47, loss = 0.07873056\n",
      "Iteration 48, loss = 0.07753757\n",
      "Iteration 49, loss = 0.07631257\n",
      "Iteration 50, loss = 0.07519702\n",
      "Iteration 51, loss = 0.07412249\n",
      "Iteration 52, loss = 0.07307190\n",
      "Iteration 53, loss = 0.07205478\n",
      "Iteration 54, loss = 0.07103166\n",
      "Iteration 55, loss = 0.07006041\n",
      "Iteration 56, loss = 0.06912731\n",
      "Iteration 57, loss = 0.06817093\n",
      "Iteration 58, loss = 0.06732803\n",
      "Iteration 59, loss = 0.06645062\n",
      "Iteration 60, loss = 0.06558962\n",
      "Iteration 61, loss = 0.06483607\n",
      "Iteration 62, loss = 0.06403956\n",
      "Iteration 63, loss = 0.06322900\n",
      "Iteration 64, loss = 0.06242733\n",
      "Iteration 65, loss = 0.06172741\n",
      "Iteration 66, loss = 0.06101365\n",
      "Iteration 67, loss = 0.06032647\n",
      "Iteration 68, loss = 0.05964161\n",
      "Iteration 69, loss = 0.05900501\n",
      "Iteration 70, loss = 0.05833741\n",
      "Iteration 71, loss = 0.05770851\n",
      "Iteration 72, loss = 0.05708546\n",
      "Iteration 73, loss = 0.05648810\n",
      "Iteration 74, loss = 0.05594438\n",
      "Iteration 75, loss = 0.05536569\n",
      "Iteration 76, loss = 0.05479816\n",
      "Iteration 77, loss = 0.05426182\n",
      "Iteration 78, loss = 0.05370089\n",
      "Iteration 79, loss = 0.05320466\n",
      "Iteration 80, loss = 0.05271749\n",
      "Iteration 81, loss = 0.05218829\n",
      "Iteration 82, loss = 0.05173711\n",
      "Iteration 83, loss = 0.05122759\n",
      "Iteration 84, loss = 0.05076121\n",
      "Iteration 85, loss = 0.05031347\n",
      "Iteration 86, loss = 0.04984951\n",
      "Iteration 87, loss = 0.04944420\n",
      "Iteration 88, loss = 0.04901305\n",
      "Iteration 89, loss = 0.04856165\n",
      "Iteration 90, loss = 0.04813583\n",
      "Iteration 91, loss = 0.04771921\n",
      "Iteration 92, loss = 0.04731069\n",
      "Iteration 93, loss = 0.04692152\n",
      "Iteration 94, loss = 0.04653405\n",
      "Iteration 95, loss = 0.04616264\n",
      "Iteration 96, loss = 0.04576042\n",
      "Iteration 97, loss = 0.04539915\n",
      "Iteration 98, loss = 0.04503159\n",
      "Iteration 99, loss = 0.04467989\n",
      "Iteration 100, loss = 0.04432119\n",
      "Iteration 101, loss = 0.04394199\n",
      "Iteration 102, loss = 0.04360018\n",
      "Iteration 103, loss = 0.04329085\n",
      "Iteration 104, loss = 0.04293800\n",
      "Iteration 105, loss = 0.04261163\n",
      "Iteration 106, loss = 0.04226391\n",
      "Iteration 107, loss = 0.04194322\n",
      "Iteration 108, loss = 0.04160244\n",
      "Iteration 109, loss = 0.04128976\n",
      "Iteration 110, loss = 0.04098502\n",
      "Iteration 111, loss = 0.04068699\n",
      "Iteration 112, loss = 0.04035014\n",
      "Iteration 113, loss = 0.04004515\n",
      "Iteration 114, loss = 0.03974372\n",
      "Iteration 115, loss = 0.03941192\n",
      "Iteration 116, loss = 0.03912987\n",
      "Iteration 117, loss = 0.03882745\n",
      "Iteration 118, loss = 0.03854753\n",
      "Iteration 119, loss = 0.03824473\n",
      "Iteration 120, loss = 0.03798549\n",
      "Iteration 121, loss = 0.03769980\n",
      "Iteration 122, loss = 0.03740947\n",
      "Iteration 123, loss = 0.03713753\n",
      "Iteration 124, loss = 0.03688842\n",
      "Iteration 125, loss = 0.03659600\n",
      "Iteration 126, loss = 0.03633920\n",
      "Iteration 127, loss = 0.03605249\n",
      "Iteration 128, loss = 0.03579604\n",
      "Iteration 129, loss = 0.03556304\n",
      "Iteration 130, loss = 0.03530831\n",
      "Iteration 131, loss = 0.03505791\n",
      "Iteration 132, loss = 0.03480213\n",
      "Iteration 133, loss = 0.03454827\n",
      "Iteration 134, loss = 0.03430384\n",
      "Iteration 135, loss = 0.03404713\n",
      "Iteration 136, loss = 0.03382092\n",
      "Iteration 137, loss = 0.03356072\n",
      "Iteration 138, loss = 0.03336322\n",
      "Iteration 139, loss = 0.03313221\n",
      "Iteration 140, loss = 0.03290290\n",
      "Iteration 141, loss = 0.03264389\n",
      "Iteration 142, loss = 0.03244174\n",
      "Iteration 143, loss = 0.03219560\n",
      "Iteration 144, loss = 0.03201243\n",
      "Iteration 145, loss = 0.03174545\n",
      "Iteration 146, loss = 0.03156087\n",
      "Iteration 147, loss = 0.03130723\n",
      "Iteration 148, loss = 0.03112155\n",
      "Iteration 149, loss = 0.03087427\n",
      "Iteration 150, loss = 0.03065379\n",
      "Iteration 151, loss = 0.03048473\n",
      "Iteration 152, loss = 0.03023207\n",
      "Iteration 153, loss = 0.03000913\n",
      "Iteration 154, loss = 0.02979166\n",
      "Iteration 155, loss = 0.02960430\n",
      "Iteration 156, loss = 0.02937553\n",
      "Iteration 157, loss = 0.02918587\n",
      "Iteration 158, loss = 0.02899045\n",
      "Iteration 159, loss = 0.02872487\n",
      "Iteration 160, loss = 0.02853528\n",
      "Iteration 161, loss = 0.02836199\n",
      "Iteration 162, loss = 0.02813926\n",
      "Iteration 163, loss = 0.02793494\n",
      "Iteration 164, loss = 0.02773074\n",
      "Iteration 165, loss = 0.02754072\n",
      "Iteration 166, loss = 0.02736594\n",
      "Iteration 167, loss = 0.02713784\n",
      "Iteration 168, loss = 0.02696510\n",
      "Iteration 169, loss = 0.02674418\n",
      "Iteration 170, loss = 0.02656135\n",
      "Iteration 171, loss = 0.02638942\n",
      "Iteration 172, loss = 0.02617125\n",
      "Iteration 173, loss = 0.02602766\n",
      "Iteration 174, loss = 0.02579618\n",
      "Iteration 175, loss = 0.02563672\n",
      "Iteration 176, loss = 0.02544247\n",
      "Iteration 177, loss = 0.02526034\n",
      "Iteration 178, loss = 0.02504893\n",
      "Iteration 179, loss = 0.02487701\n",
      "Iteration 180, loss = 0.02471041\n",
      "Iteration 181, loss = 0.02449422\n",
      "Iteration 182, loss = 0.02431900\n",
      "Iteration 183, loss = 0.02418137\n",
      "Iteration 184, loss = 0.02398443\n",
      "Iteration 185, loss = 0.02379151\n",
      "Iteration 186, loss = 0.02363551\n",
      "Iteration 187, loss = 0.02345619\n",
      "Iteration 188, loss = 0.02325672\n",
      "Iteration 189, loss = 0.02312396\n",
      "Iteration 190, loss = 0.02292155\n",
      "Iteration 191, loss = 0.02274389\n",
      "Iteration 192, loss = 0.02258448\n",
      "Iteration 193, loss = 0.02245008\n",
      "Iteration 194, loss = 0.02227490\n",
      "Iteration 195, loss = 0.02209154\n",
      "Iteration 196, loss = 0.02192439\n",
      "Iteration 197, loss = 0.02176588\n",
      "Iteration 198, loss = 0.02162077\n",
      "Iteration 199, loss = 0.02144009\n",
      "Iteration 200, loss = 0.02130010\n",
      "Iteration 201, loss = 0.02112666\n",
      "Iteration 202, loss = 0.02097698\n",
      "Iteration 203, loss = 0.02084637\n",
      "Iteration 204, loss = 0.02064789\n",
      "Iteration 205, loss = 0.02048854\n",
      "Iteration 206, loss = 0.02034941\n",
      "Iteration 207, loss = 0.02018657\n",
      "Iteration 208, loss = 0.02003814\n",
      "Iteration 209, loss = 0.01987348\n",
      "Iteration 210, loss = 0.01970949\n",
      "Iteration 211, loss = 0.01956141\n",
      "Iteration 212, loss = 0.01941452\n",
      "Iteration 213, loss = 0.01928984\n",
      "Iteration 214, loss = 0.01912799\n",
      "Iteration 215, loss = 0.01898896\n",
      "Iteration 216, loss = 0.01883122\n",
      "Iteration 217, loss = 0.01871677\n",
      "Iteration 218, loss = 0.01858401\n",
      "Iteration 219, loss = 0.01843153\n",
      "Iteration 220, loss = 0.01828506\n",
      "Iteration 221, loss = 0.01816709\n",
      "Iteration 222, loss = 0.01802849\n",
      "Iteration 223, loss = 0.01787034\n",
      "Iteration 224, loss = 0.01774518\n",
      "Iteration 225, loss = 0.01763802\n",
      "Iteration 226, loss = 0.01749969\n",
      "Iteration 227, loss = 0.01735283\n",
      "Iteration 228, loss = 0.01722513\n",
      "Iteration 229, loss = 0.01711557\n",
      "Iteration 230, loss = 0.01698118\n",
      "Iteration 231, loss = 0.01686933\n",
      "Iteration 232, loss = 0.01671678\n",
      "Iteration 233, loss = 0.01661609\n",
      "Iteration 234, loss = 0.01649031\n",
      "Iteration 235, loss = 0.01635968\n",
      "Iteration 236, loss = 0.01625179\n",
      "Iteration 237, loss = 0.01612991\n",
      "Iteration 238, loss = 0.01599829\n",
      "Iteration 239, loss = 0.01590206\n",
      "Iteration 240, loss = 0.01579040\n",
      "Iteration 241, loss = 0.01565565\n",
      "Iteration 242, loss = 0.01553486\n",
      "Iteration 243, loss = 0.01541987\n",
      "Iteration 244, loss = 0.01533116\n",
      "Iteration 245, loss = 0.01521126\n",
      "Iteration 246, loss = 0.01509990\n",
      "Iteration 247, loss = 0.01497294\n",
      "Iteration 248, loss = 0.01487199\n",
      "Iteration 249, loss = 0.01477178\n",
      "Iteration 250, loss = 0.01465563\n",
      "Iteration 251, loss = 0.01455440\n",
      "Iteration 252, loss = 0.01444511\n",
      "Iteration 253, loss = 0.01434280\n",
      "Iteration 254, loss = 0.01423811\n",
      "Iteration 255, loss = 0.01412886\n",
      "Iteration 256, loss = 0.01403272\n",
      "Iteration 257, loss = 0.01392633\n",
      "Iteration 258, loss = 0.01382203\n",
      "Iteration 259, loss = 0.01374349\n",
      "Iteration 260, loss = 0.01362897\n",
      "Iteration 261, loss = 0.01353483\n",
      "Iteration 262, loss = 0.01344195\n",
      "Iteration 263, loss = 0.01333407\n",
      "Iteration 264, loss = 0.01325282\n",
      "Iteration 265, loss = 0.01314746\n",
      "Iteration 266, loss = 0.01306878\n",
      "Iteration 267, loss = 0.01295755\n",
      "Iteration 268, loss = 0.01287522\n",
      "Iteration 269, loss = 0.01278763\n",
      "Iteration 270, loss = 0.01269527\n",
      "Iteration 271, loss = 0.01261049\n",
      "Iteration 272, loss = 0.01252395\n",
      "Iteration 273, loss = 0.01243520\n",
      "Iteration 274, loss = 0.01234405\n",
      "Iteration 275, loss = 0.01226209\n",
      "Iteration 276, loss = 0.01217704\n",
      "Iteration 277, loss = 0.01209534\n",
      "Iteration 278, loss = 0.01200121\n",
      "Iteration 279, loss = 0.01192151\n",
      "Iteration 280, loss = 0.01183424\n",
      "Iteration 281, loss = 0.01175622\n",
      "Iteration 282, loss = 0.01168598\n",
      "Iteration 283, loss = 0.01160958\n",
      "Iteration 284, loss = 0.01152004\n",
      "Iteration 285, loss = 0.01143070\n",
      "Iteration 286, loss = 0.01136168\n",
      "Iteration 287, loss = 0.01130825\n",
      "Iteration 288, loss = 0.01120488\n",
      "Iteration 289, loss = 0.01113485\n",
      "Iteration 290, loss = 0.01105606\n",
      "Iteration 291, loss = 0.01098444\n",
      "Iteration 292, loss = 0.01091609\n",
      "Iteration 293, loss = 0.01083816\n",
      "Iteration 294, loss = 0.01076259\n",
      "Iteration 295, loss = 0.01069604\n",
      "Iteration 296, loss = 0.01062601\n",
      "Iteration 297, loss = 0.01054560\n",
      "Iteration 298, loss = 0.01048188\n",
      "Iteration 299, loss = 0.01041244\n",
      "Iteration 300, loss = 0.01034847\n",
      "Iteration 301, loss = 0.01026784\n",
      "Iteration 302, loss = 0.01020502\n",
      "Iteration 303, loss = 0.01014663\n",
      "Iteration 304, loss = 0.01007170\n",
      "Iteration 305, loss = 0.00999840\n",
      "Iteration 306, loss = 0.00994360\n",
      "Iteration 307, loss = 0.00987934\n",
      "Iteration 308, loss = 0.00979841\n",
      "Iteration 309, loss = 0.00974565\n",
      "Iteration 310, loss = 0.00967874\n",
      "Iteration 311, loss = 0.00961842\n",
      "Iteration 312, loss = 0.00955233\n",
      "Iteration 313, loss = 0.00949338\n",
      "Iteration 314, loss = 0.00943633\n",
      "Iteration 315, loss = 0.00936993\n",
      "Iteration 316, loss = 0.00930779\n",
      "Iteration 317, loss = 0.00925134\n",
      "Iteration 318, loss = 0.00919000\n",
      "Iteration 319, loss = 0.00912418\n",
      "Iteration 320, loss = 0.00907935\n",
      "Iteration 321, loss = 0.00901694\n",
      "Iteration 322, loss = 0.00895158\n",
      "Iteration 323, loss = 0.00889937\n",
      "Iteration 324, loss = 0.00885096\n",
      "Iteration 325, loss = 0.00878724\n",
      "Iteration 326, loss = 0.00872800\n",
      "Iteration 327, loss = 0.00867817\n",
      "Iteration 328, loss = 0.00861758\n",
      "Iteration 329, loss = 0.00857728\n",
      "Iteration 330, loss = 0.00852492\n",
      "Iteration 331, loss = 0.00846712\n",
      "Iteration 332, loss = 0.00840699\n",
      "Iteration 333, loss = 0.00836396\n",
      "Iteration 334, loss = 0.00830279\n",
      "Iteration 335, loss = 0.00825370\n",
      "Iteration 336, loss = 0.00820052\n",
      "Iteration 337, loss = 0.00815251\n",
      "Iteration 338, loss = 0.00810470\n",
      "Iteration 339, loss = 0.00804920\n",
      "Iteration 340, loss = 0.00800151\n",
      "Iteration 341, loss = 0.00795286\n",
      "Iteration 342, loss = 0.00790897\n",
      "Iteration 343, loss = 0.00786098\n",
      "Iteration 344, loss = 0.00780987\n",
      "Iteration 345, loss = 0.00776189\n",
      "Iteration 346, loss = 0.00771384\n",
      "Iteration 347, loss = 0.00766916\n",
      "Iteration 348, loss = 0.00762412\n",
      "Iteration 349, loss = 0.00757821\n",
      "Iteration 350, loss = 0.00754030\n",
      "Iteration 351, loss = 0.00748372\n",
      "Iteration 352, loss = 0.00743702\n",
      "Iteration 353, loss = 0.00739810\n",
      "Iteration 354, loss = 0.00735955\n",
      "Iteration 355, loss = 0.00731444\n",
      "Iteration 356, loss = 0.00727040\n",
      "Iteration 357, loss = 0.00722637\n",
      "Iteration 358, loss = 0.00719312\n",
      "Iteration 359, loss = 0.00714873\n",
      "Iteration 360, loss = 0.00710138\n",
      "Iteration 361, loss = 0.00706458\n",
      "Iteration 362, loss = 0.00702046\n",
      "Iteration 363, loss = 0.00698443\n",
      "Iteration 364, loss = 0.00694095\n",
      "Iteration 365, loss = 0.00690524\n",
      "Iteration 366, loss = 0.00687270\n",
      "Iteration 367, loss = 0.00682676\n",
      "Iteration 368, loss = 0.00678619\n",
      "Iteration 369, loss = 0.00675136\n",
      "Iteration 370, loss = 0.00670880\n",
      "Iteration 371, loss = 0.00667657\n",
      "Iteration 372, loss = 0.00663524\n",
      "Iteration 373, loss = 0.00659809\n",
      "Iteration 374, loss = 0.00657188\n",
      "Iteration 375, loss = 0.00652714\n",
      "Iteration 376, loss = 0.00649223\n",
      "Iteration 377, loss = 0.00645146\n",
      "Iteration 378, loss = 0.00641819\n",
      "Iteration 379, loss = 0.00638534\n",
      "Iteration 380, loss = 0.00634560\n",
      "Iteration 381, loss = 0.00631144\n",
      "Iteration 382, loss = 0.00627330\n",
      "Iteration 383, loss = 0.00624091\n",
      "Iteration 384, loss = 0.00621086\n",
      "Iteration 385, loss = 0.00617058\n",
      "Iteration 386, loss = 0.00613750\n",
      "Iteration 387, loss = 0.00610226\n",
      "Iteration 388, loss = 0.00606523\n",
      "Iteration 389, loss = 0.00603528\n",
      "Iteration 390, loss = 0.00600675\n",
      "Iteration 391, loss = 0.00597531\n",
      "Iteration 392, loss = 0.00593768\n",
      "Iteration 393, loss = 0.00591272\n",
      "Iteration 394, loss = 0.00587608\n",
      "Iteration 395, loss = 0.00584571\n",
      "Iteration 396, loss = 0.00581295\n",
      "Iteration 397, loss = 0.00578159\n",
      "Iteration 398, loss = 0.00575385\n",
      "Iteration 399, loss = 0.00571769\n",
      "Iteration 400, loss = 0.00569214\n",
      "Iteration 401, loss = 0.00566503\n",
      "Iteration 402, loss = 0.00563338\n",
      "Iteration 403, loss = 0.00560492\n",
      "Iteration 404, loss = 0.00556711\n",
      "Iteration 405, loss = 0.00554155\n",
      "Iteration 406, loss = 0.00550764\n",
      "Iteration 407, loss = 0.00547759\n",
      "Iteration 408, loss = 0.00545035\n",
      "Iteration 409, loss = 0.00541925\n",
      "Iteration 410, loss = 0.00540000\n",
      "Iteration 411, loss = 0.00536631\n",
      "Iteration 412, loss = 0.00533704\n",
      "Iteration 413, loss = 0.00530934\n",
      "Iteration 414, loss = 0.00528012\n",
      "Iteration 415, loss = 0.00525351\n",
      "Iteration 416, loss = 0.00522493\n",
      "Iteration 417, loss = 0.00519941\n",
      "Iteration 418, loss = 0.00517300\n",
      "Iteration 419, loss = 0.00514599\n",
      "Iteration 420, loss = 0.00511819\n",
      "Iteration 421, loss = 0.00509047\n",
      "Iteration 422, loss = 0.00506958\n",
      "Iteration 423, loss = 0.00503841\n",
      "Iteration 424, loss = 0.00501580\n",
      "Iteration 425, loss = 0.00499700\n",
      "Iteration 426, loss = 0.00496295\n",
      "Iteration 427, loss = 0.00493702\n",
      "Iteration 428, loss = 0.00491625\n",
      "Iteration 429, loss = 0.00488809\n",
      "Iteration 430, loss = 0.00486028\n",
      "Iteration 431, loss = 0.00483885\n",
      "Iteration 432, loss = 0.00481148\n",
      "Iteration 433, loss = 0.00478865\n",
      "Iteration 434, loss = 0.00476266\n",
      "Iteration 435, loss = 0.00474067\n",
      "Iteration 436, loss = 0.00471582\n",
      "Iteration 437, loss = 0.00468998\n",
      "Iteration 438, loss = 0.00466716\n",
      "Iteration 439, loss = 0.00464590\n",
      "Iteration 440, loss = 0.00462277\n",
      "Iteration 441, loss = 0.00460028\n",
      "Iteration 442, loss = 0.00457729\n",
      "Iteration 443, loss = 0.00455599\n",
      "Iteration 444, loss = 0.00453419\n",
      "Iteration 445, loss = 0.00450743\n",
      "Iteration 446, loss = 0.00448647\n",
      "Iteration 447, loss = 0.00446365\n",
      "Iteration 448, loss = 0.00444144\n",
      "Iteration 449, loss = 0.00441963\n",
      "Iteration 450, loss = 0.00439952\n",
      "Iteration 451, loss = 0.00437675\n",
      "Iteration 452, loss = 0.00435636\n",
      "Iteration 453, loss = 0.00433531\n",
      "Iteration 454, loss = 0.00431240\n",
      "Iteration 455, loss = 0.00429470\n",
      "Iteration 456, loss = 0.00427161\n",
      "Iteration 457, loss = 0.00425072\n",
      "Iteration 458, loss = 0.00423279\n",
      "Iteration 459, loss = 0.00420946\n",
      "Iteration 460, loss = 0.00418905\n",
      "Iteration 461, loss = 0.00417015\n",
      "Iteration 462, loss = 0.00415022\n",
      "Iteration 463, loss = 0.00412937\n",
      "Iteration 464, loss = 0.00411196\n",
      "Iteration 465, loss = 0.00409006\n",
      "Iteration 466, loss = 0.00407064\n",
      "Iteration 467, loss = 0.00405329\n",
      "Iteration 468, loss = 0.00403367\n",
      "Iteration 469, loss = 0.00401435\n",
      "Iteration 470, loss = 0.00399516\n",
      "Iteration 471, loss = 0.00397583\n",
      "Iteration 472, loss = 0.00395483\n",
      "Iteration 473, loss = 0.00394100\n",
      "Iteration 474, loss = 0.00392139\n",
      "Iteration 475, loss = 0.00390292\n",
      "Iteration 476, loss = 0.00388481\n",
      "Iteration 477, loss = 0.00386463\n",
      "Iteration 478, loss = 0.00384622\n",
      "Iteration 479, loss = 0.00383042\n",
      "Iteration 480, loss = 0.00381635\n",
      "Iteration 481, loss = 0.00379384\n",
      "Iteration 482, loss = 0.00377687\n",
      "Iteration 483, loss = 0.00375954\n",
      "Iteration 484, loss = 0.00373992\n",
      "Iteration 485, loss = 0.00372797\n",
      "Iteration 486, loss = 0.00370725\n",
      "Iteration 487, loss = 0.00368949\n",
      "Iteration 488, loss = 0.00367616\n",
      "Iteration 489, loss = 0.00365694\n",
      "Iteration 490, loss = 0.00364170\n",
      "Iteration 491, loss = 0.00362382\n",
      "Iteration 492, loss = 0.00360575\n",
      "Iteration 493, loss = 0.00359105\n",
      "Iteration 494, loss = 0.00357355\n",
      "Iteration 495, loss = 0.00355763\n",
      "Iteration 496, loss = 0.00354238\n",
      "Iteration 497, loss = 0.00352623\n",
      "Iteration 498, loss = 0.00350909\n",
      "Iteration 499, loss = 0.00349321\n",
      "Iteration 500, loss = 0.00347996\n",
      "[[ 60   3]\n",
      " [  3 105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        63\n",
      "           1       0.97      0.97      0.97       108\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.96      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KING-OBI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=0.0001,\n",
    "                    solver='adam', verbose=10, random_state=21, tol=0.000000001)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b170544-5f1a-4170-819a-cab59aa805e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
